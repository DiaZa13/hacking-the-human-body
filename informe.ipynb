{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "d095FG5MmBAJIPchdCxrqg",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Las unidades de tejido funcional tienen gran importancia al conectar el ser humano como sistema y las células como unidad básica de funcionamiento. A su vez, son relevantes a nivel patobiológico, ya que, según Godwin en *Robust and generalizable segmentation of human functional tissue units* las unidades de tejido funcional permiten modelar y comprender la progresión de enfermedades en el ser humano. Al tener un mayor conocimiento sobre las áreas en las cuales se encuentran las unidades de tejido funcional lo que se busca es mejorar y prolongar la vida humana, a través de la información sobre la función de las células que tienen un papel importante en la salud. Sin embargo, a pesar de su relevancia en el área médica aún existe una brecha de conocimiento de la locación de las unidades funcionales en diferentes órganos, esto principalmente debido a que es una actividad costosa, ya que requiere mucho tiempo. En promedio un patólogo entrenado necesita 10 horas para encontrar alrededor de 200 unidades funcionales en un órgano, esto no es eficiente si se toma en consideración que solamente en un riñón están presentes alrededor de 1 millón de unidades funcionales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "FRw6thbgYwfEZJYOFNcQrX",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Objetivos\n",
    "**Objetivos Generales**\n",
    "- Construir un modelo generalizable que identifique unidades de tejido funcional para facilitar investigaciones sobre la localización de estas células en un órgano.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "- Generar un modelo que utilize visión artificial para detectar si un tejido funcional proviene de la próstata, bazo, pulmones, riñones o intestino grueso con la mayor precisión posible.\n",
    "- Generar un modelo para detectar el órgano del que proviene un tejido funcional que tome la menor cantidad de tiempo para analizar un tejido específico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "e95LPxF8KSxeiA4Gwhe8s6",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Procesamiento de los datos\n",
    "El preprocesamiento de imágenes es al análisis de imágenes lo que la normalización de datos es al análisis numérico. Si bien el uso de estos métodos es en su naturaleza una distorsión de las imágenes a trabajar, puede que este proceso lleve a una mejora en la detección de características dentro de los modelos a utilizar.\n",
    "\n",
    "El método de preprocesamiento de imagen a utilizar puede depender de como se obtuvo la data a estudiar. Debe tomarse en cuenta el equipo utilizado para generar las imágenes, así como el formato y características de las imágenes en sí. Una de las distinciones mas simple es si se trabaja con imágenes a color o en blanco y negro. En general podemos dividir los métodos de preprocesamiento de imágenes en cuatro categorías:\n",
    "\n",
    "- Descriptores locales binarios: Este tipo de métodos se centra en comparar la intensidad de pixeles a pares. Esto hace que las comparaciones sean relativamente insensibles a iluminación, brillo y contraste, por lo que puede que no sea necesario un preprocesamiento extenso para obtener buenos resultados.\n",
    "- Descriptores espectrales: Este tipo de métodos ofrece oportunidades de preprocesamiento más variadas. Este tipo de métodos trabaja con pirámides de imágenes, busca corregir y filtrar la iluminación de las imágenes y mejorar contrastes para mejor detección de características en las imágenes.\n",
    "- Descriptores de espacio básico:  Estos métodos pueden trabajar de manera global o local. Generan o distinguen formas básicas dentro de las imágenes. Estas pueden generarse a partir de invariantes de rotación dentro de las imágenes.\n",
    "- Descriptores de formas poligonales: Esta familia de métodos puede considerarse la mas compleja. Esto se debe a que pueden requerirse múltiples técnicas de preprocesamiento para lograr distinguir las formas polinomiales dentro de las imágenes. Este proceso puede ir desde el mejoramiento de la imagen hasta transformaciones morfológicas de la misma. Este proceso se enfoca en amplias regiones dentro de las imágenes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "veAfseeYJos3t1rqaBLLt9",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "## Información teórica sobre análisis de los datos desde el punto de vista de los expertos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "hPy9POhMp191WjjVg9YbU8",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Algoritmo de aprendizaje de máquinas\n",
    "Como bien se detalló con anterioridad el problema consiste en poder determinar las unidades de tejido funcionale en una imagen de una célula, ya sea de hígado, intestino grueso, pulmones, bazo o próstata. Para ello no es suficiente utilizar técnicas de clasificación convencionales ya que no se busca clasificar la imagen en general, sino por su parte, se busca hacer una clasificación de cada uno de los pixeles de la imagen, para esto se utilizará algo conocido como *Image Segmentation* \n",
    "\n",
    "## Image segmentation\n",
    "\n",
    "Esta técnica consiste en la partición de una imagen en grupos más pequeños de la misma llamadas \"image segments\". Esto con el fin de poder reducir la complejidad de la imagen y dar la posibilidad de realizar análisis más profundo sobre cada segmento. \n",
    "\n",
    "Un uso común que se le da a esta técnica es el de poder detectar patrones en los segmentos. Esto ocasiona que se puedan detectar objetos en las imagenes sin la necesidad de analizar la imagen completa. Gracias a esto se logra un mejor rendimiento y precisión a la hora de analizar imagenes. \n",
    "\n",
    "## ¿Cómo funciona la técnica Image Segmentation?\n",
    "\n",
    "Esta técnica toma como entrada una imagen y produce como salida una mascara o matriz con varios elementos clasificados, ya sea en clases o en instancias de donde pertenece cada pixel.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://mlv4xkdrf2yq.i.optimole.com/cb:0l1_~a759/w:1024/h:771/q:mauto/https://datagen.tech/app/uploads/2022/05/image6-2.png\" width=\"600\" height=\"400\">\n",
    "  <p align=\"center\" >(Datagen, sf)</p>\n",
    "</p>\n",
    "\n",
    "## Tipos de algoritmos de segmentación de imágenes\n",
    "\n",
    "En la actualidad, se conocen tres tipos de métodos para realizar segmentación de imágenes:\n",
    "\n",
    "- **Semántica**: este tipo asocia cada pixel de la imágen a una clase en específico (i.e carros, plantas, personas, etc). Toma múltiples objetos de la misma clase como una sola identidad. \n",
    "- **Insancia**: a diferencia de la semántica, que toma varios objetos como uno solo, este toma múltiples objetos como individuos distintos. \n",
    "- **Panoptic**: Este tipo es la combinación de los dos anteriores, semántica e instancia. Cada objeto es representado por una clase y cada objeto es tomado como un individuo distinto.\n",
    "(Shiledarbaxi, N. sf)\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/6124a3554942a64e5edd7f20_Classification%20Detection.png\" width=\"600\" height=\"400\">\n",
    "  <p align=\"center\" >(Bandyopadhyay. H, 10/2022)</p>\n",
    "</p>\n",
    "\n",
    "## Técnicas de segmentación \n",
    "Al igual que existen varios tipos de segmentación, también a su vez, existen una diversidad de técnicas para predecir la clasificación de los pixeles de una imagen\n",
    "\n",
    "## K-Means\n",
    "\n",
    "K-Means es un algoritmo no supervisado. Su finalidad es la de dividir la información en distintas clases o clusters basados en la información que posee cada objeto. \n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "- Se escoge el valor de k, el cual es el número de clusters que se usaran.\n",
    "- Asignar los objetos de forma aleatoria a cualquiera de los k clusters\n",
    "- Calcular el centro de cada cluster\n",
    "- Calcular la distancia de cada objeto hacia el centro de cada cluster\n",
    "- Asignar los objetos al cluster con el que posean la distancia más corta hacia su centro\n",
    "- Calcular nuevamente el centro de cada cluster\n",
    "- Repetir los tres pasos anteriores hasta que los objetos no cambien de cluster.\n",
    "\n",
    "<p align=\"right\" >(GeeksForGeeks, 07/2021)</p>\n",
    "\n",
    "## Otsu´s method\n",
    "\n",
    "Este algoritmo tiene la finalidad de poder realizar la umbralización de una imagén. La umbralización consiste en la separación de los objetos de interes con el fondo, esto se realiza a partir de un valor de \"umbral\". Este valor de umbral se puede colocar como un número al azar, pero existen diversos métodos para poder encontrar un valor óptimo de umbral. Uno de estos el método de Otsu. \n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "- Procesar la imagen de entrada\n",
    "- Obtener la distribución de los pixeles de la imagen (puede ser por un histograma)\n",
    "- Calcular el valor de umbral T\n",
    "    - Este valor se calcula por medio de la dispersión que hay de cada segmento, tomando el cociente entre las varianzas de cada segmento y se busca un valor de umbral en el que el cociente de este sea lo máximo posible (en la referencia se puede ver una explicación más detallada de como calcular este valor).\n",
    "- Remplazar los pixeles a pixeles de color blanco en donde la saturación es mayor a T y de color negro aquellos pixeles que posean una saturación menor a T. \n",
    "\n",
    "<p align=\"center\" >\n",
    "    <img src=\"https://muthu.co/wp-content/uploads/2020/03/download-1.png\" width=\"600\" height=\"400\">\n",
    "    <p align=\"center\">(Murzova. A, 08/2020)</p>\n",
    "</p>\n",
    "\n",
    "## Region-Based Segmentation\n",
    "\n",
    "Este algoritmo se enfoca en la busqueda de características similares entre los pixeles y sus adyacentes, con el fin de agruparlos en una clase en común. \n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "Este algoritmo inicia definiendo a unos pixeles como \"seed pixels\". Luego de definir estos pixeles, calcula los límites que poseen estos pixeles y clasifica los pixeles que tiene alrededor como similares o como distintos. Luego se escogen los vecinos próximos como los siguientes \"seed pixels\" y se repite todo el proceso hasta que toda la imagen esté segmentada. \n",
    "(Bandyopadhyay. H, 10/2022)\n",
    "\n",
    "## Canny Edge detection\n",
    "\n",
    "Este algoritmo busca poder detectar los bordes de una imágen para la separación de los objetos.\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "- Reducir el ruido que pueda poseer la imágen\n",
    "- Calcular la gradiente de la imagen. Esto se hace por medio de analizar los saltos de intensidad que hay entre cada pixel\n",
    "- Realizar un adelgalzamiento de los bordes.\n",
    "- Colocar un doble umbral, con la finalidad de poder determinar tres tipos de pixeles\n",
    "    - Pixeles fuertes: Son pixeles que poseen una intensidad alta, los cuales contribuyen al borde final\n",
    "    - Pixeles débiles: Son pixeles que no poseen una intensidad tan alta como considerarse pixeles fuertes, pero siguen siendo pixeles que aportan información a la imagen. \n",
    "    - Pixeles no relevantes: Son todos aquellos pixeles con una intensidad tan baja que no son considerados relevantes en la imagen.\n",
    "- Busqueda de bordes. Esto se realizar por medio de analizar los pixeles débiles con la finalidad de determinar si alguno de estos se puede considerar como un pixel fuerte. \n",
    "\n",
    "(En la referencia se puede ver una explicación más detallada de cada paso)\n",
    "\n",
    "<p align=\"center\" >\n",
    "    <img src=\"https://miro.medium.com/max/640/1*ZCyKWsmDoj6V-dNwKlKxyA.png\" width=\"600\" height=\"400\">\n",
    "    <p align=\"center\" >(Sahir. S, 01/2019)</p>\n",
    "</p>\n",
    "\n",
    "\n",
    "## Redes convolucionales\n",
    "\n",
    "Las redes convolucionales son un tipo de redes neuronales en donde las neuronas representan a las neuronas de la corteza visual primaria de un cerebro biológico. Estas redes son utilizadas para trabajar con imágenes. \n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "La red convolucional extrae automáticamente caracterísitcas de los datos de entrenamiento. Dichas caracterísitcas servirán para poder clasificar cada objeto. Estas características las logra identificar a partir de patrones, bordes, lineas o formas que se cruzan. De igual forma como trabaja la corteza visual de los mamíferos. \n",
    "\n",
    "(KeepCodig, 01/2022)\n",
    "\n",
    "<p align=\"center\" >\n",
    "    <img src=\"https://www.iartificial.net/wp-content/uploads/2021/04/20210418_001045-1024x284.webp\" width=\"600\" height=\"400\">\n",
    "    <p align=\"center\">(Iglesias. D, 05/2021)</p>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "GUhrdCGwXYgFWe9nPKKuH2",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "## Metodología\n",
    "A continuación se detalla los pasos que se siguieron, tanto para elaborar los modelos de segmentación como la preparación de los conjuntos tanto de entrenamiento como de prueba.\n",
    "\n",
    "### Pasos para resolver el problema\n",
    "Explicación de cómo seleccionó el conjunto de entrenamiento y prueba\n",
    "\n",
    "Debido a que se tenía un conjunto de datos relativamente pequeño, o al menos para trabajar con redes convolucionales, se dió prioridad al conjunto de entrenamiento, en este caso se decidió que un 95% de los datos se utilizaría para entrenamiento, y de estos un 13% se usaría para validación. La separación de los datos se hizo mediante la función *train_test_split* de la librería Sklearn en conjunto con pandas. Para el uso de pandas se tomó el conjunto de datos en formato .csv y de estos se tomó el id de la imagen así como el rle correspondiente de las máscaras. Una vez se obtuvieron los respectivos conjuntos de entrenamiento y prueba, debido a que en el dataset el id únicamente contiene el número y no el formato, para facilitar posteriormente la lectura de las imágenes se agregó a cada observación el formato de las imágenes, siendo, en este caso .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "VjRw8xoDl2OkL2pM4WBGA6",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# to convert the masks to images it is necessary to return also the mask test and train series\n",
    "def get_train_test(filename: str):\n",
    "    # separate the data into test and train\n",
    "    data = pd.read_csv(filename)\n",
    "    images = data[['id']]\n",
    "    masks = data['rle']\n",
    "    images_train, images_test, masks_train, masks_test = train_test_split(images, masks, test_size=0.05, random_state=SEED)\n",
    "    # adding the extension to the id images\n",
    "    images_train['id'] = images_train['id'].apply(lambda x: str(x) + '.png')\n",
    "    images_test['id'] = images_test['id'].apply(lambda x: str(x) + '.png')\n",
    "    # reset the index\n",
    "    images_train.reset_index(inplace=True, drop=True)\n",
    "    images_test.reset_index(inplace=True, drop=True)\n",
    "    masks_train.reset_index(inplace=True, drop=True)\n",
    "    masks_test.reset_index(inplace=True, drop=True)\n",
    "    return images_train, images_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "pblv3BYSweQXjiQEnNOk2p",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# get train and test\n",
    "train, test = get_train_test('train.csv')\n",
    "\n",
    "# train dataset\n",
    "image_generator = augmentation(AUG_TRAIN_ARGS, train, IMAGES_PATH, IMG_SIZE, 'id', BATCH_SIZE, SEED, 'training')\n",
    "mask_generator = augmentation(AUG_TRAIN_ARGS, train, MASKS_PATH, IMG_SIZE, 'id', BATCH_SIZE, SEED, 'training')\n",
    "# combine generators into one which yields image and masks\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "train_size = image_generator.samples\n",
    "\n",
    "# validation\n",
    "image_generator = augmentation(AUG_TRAIN_ARGS, train, IMAGES_PATH, IMG_SIZE, 'id', BATCH_SIZE, SEED, 'validation')\n",
    "mask_generator = augmentation(AUG_TRAIN_ARGS, train, MASKS_PATH, IMG_SIZE, 'id', BATCH_SIZE, SEED, 'validation')\n",
    "# combine generators into one which yields image and masks\n",
    "validation_generator = zip(image_generator, mask_generator)\n",
    "validation_size = image_generator.samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KtEbb8fFi9UGZC0rUqKxZY",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "### Selección de los algoritmos\n",
    "Explicación de la selección de los algoritmos y las razones por las se escogieron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "kWMMz9vIp3qWk3utUbe7kJ",
     "type": "MD"
    }
   },
   "source": [
    "Se trabajó con redes convolucionales, sin embargo se realizaron dos arquitecturas diferentes\n",
    "\n",
    "\n",
    "UNet → Se decidió utilizar esta arquitectura debido a que este fue desarrollado para trabajar con imágenes biomédicas, lo cual lo hace perfecto para los objetivos de este proyecto. Además, esta arquitectura puede trabar con datasets pequeños, por lo que lo hace una muy buena opción para este proyecto. Y por último, esta arquitectura es muy utilizado, por lo que existe buena documentación en internet sobre como funciona. \n",
    "\n",
    "\n",
    "Linknet *FCN - Fast Fully Connected Network* → La principal razón por la cual se utilizó esta arquitectura es debido a su eficiencia. Linknet da resultados de una forma rápida, sin comprometer el resultado del accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "d422VcsFTB5lZmbookkptD",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "### Explicación de selección de las herramientas utilizadas\n",
    "\n",
    "#### Recursos de cómputo\n",
    "  - Computadoras del UX Lab de la universidad del Valle de Guatemala\n",
    "    - Procesador: Intel Core i7 11th Generación\n",
    "    - RAM: 32GB \n",
    "    - Tarjeta de video: Nvidia GeForce RTX 4000\n",
    "  - Laptop MSI Stealth\n",
    "    - Procesador: Intel Core i7-11375H\n",
    "    - RAM: 16GB DDR4\n",
    "    - Tarjeta de video: Nvidia GeForce RTX 3060\n",
    "  - Laptop Acer Nitro 5 AN515-51\n",
    "    - Procesador: Intel Core i7-7700HQ\n",
    "    - RAM: 32GB \n",
    "    - Tarjeta de video: Nvidia GeForce GTX 1050 Ti\n",
    "\n",
    "#### Bibliotecas y/o paquetes utilizados\n",
    "\n",
    "  En cuanto a las librerías utilizadas, los modelos se desarrollaron princpalmente en Tensorflow, esto principalmente a que es una librería que cuenta con vasto soporte para trabajar con redes neuronales y sumado a esto, ha sido la librería que se ha utilizado a lo largo del curso, por lo que ya se está familiarizado con la estructura, funciones y argumentos de esta. \n",
    "  \n",
    "  Por su parte, también se utilizó una librería que ya cuenta con modelos de segmentación pre-entrenados, la libería se llama Segmentation Models, está desarrollada en Keras y Tensorflow y esta asegura disminuir y mejorar los resultados de los modelos de segmentación realizados mediante la librería mediante diferentes *backbones*. Estos backbones son una arquitectura que definen como las distintas capas de la red llegarán a la capa de encoder y como serán construídas de nuevo en el decoder. En la práctica se utilizaron 4 *backbones* y se pudo observar una mejora significativa tanto en tiempo como en *accuracy* en aquellos *backbones* de la parte de *EfficientNet* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "NPi4vZYV66Jhu1RLDpuKiA",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Resultados y Análisis de Resultados\n",
    "\n",
    "## Conjunto de datos\n",
    "Descripción de las características del conjunto de datos\n",
    "\n",
    "El conjunto de datos para la elaboración de análisis de los tejidos funcionales de los órganos fue obtenido mediante la página de Kaggle de la competencia *Hacking the Human Body*. Los datos proporcionados constan de 351 observaciones (con sus respectivas imágenes para el análisis) y 10 variables, en las cuales se describe tanto información de las imágenes como el sexo y la edad del donador. Los organos provistos para el análisis son: riñón, pulmón, bazo, intestino grueso y próstata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "PujvF2bEDJplcQxiFcijde",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "OlzeUs4gmzlQS9yETgKBzT",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "## Preprocesamiento\n",
    "Descripción de las tareas de limpieza y preprocesamiento a las que tuvo que someter a los datos para lograr los resultados obtenidos\n",
    "\n",
    "Debido a que se está trabajando con imágenes, no es necesario realizar muchas operaciones de limpieza de los datos, sin embargo, en este caso como se está trabajando, tanto con imágenes como con sus respectivas máscaras, se tuvo que transformar los rle en los cuales se encontraban las unidades de tejido funcional a imágenes y se almacenaron en una carpeta para que luego estas pudieran ser accesadas en tiempo real por los modelos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ecpOd6umQsgWCZSCXOc5Bx",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def rle2mask(rle, shape):\n",
    "    \"\"\"\n",
    "        rle: run-length as string formatted (start length)\n",
    "        shape: (height,width) of array to return\n",
    "        Returns numpy array, 1 → mask, 0 → background\n",
    "    \"\"\"\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 255\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "\n",
    "def save_masks(images: [str], masks: [str], src_dir: str, dst_dir: str):\n",
    "    for index, image in tqdm(enumerate(images), total=len(images)):\n",
    "        img = Image.open(src_dir + image)\n",
    "        mask = rle2mask(masks[index], img.size[:2])\n",
    "        img.save(f'{dst_dir}/{image}', mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ZyEIAi0Hxppf0XZtvr4zdo",
     "type": "MD"
    }
   },
   "source": [
    "Una vez se tuvo tanto las imágenes como las respectivas máscaras fue necesario \"generar\" una mayor cantidad de imágenes, ya que como se comentó, únicamente se cuenta con 351. Para ello se utilizó la técnica conocido como *data augmentation* esta se realizó mediante la librería Keras y su función *ImageDataGenerator*. Algo interesante es que el \"aumentador\" ya cuenta con la opción para poder normalizar las imágenes, lo cual facilitó el preprocesamiento de las mismas. El conjunto de transformaciones que se establecieron fueron:\n",
    "- Rotación: 35º\n",
    "- Voltear horizontalmente\n",
    "- Voltear verticalmente\n",
    "- Alargamiento en el ancho de máximo 0.5% de la imagen\n",
    "- Alargamiento en el largo de máximo un 0.5% de la imagen\n",
    "- Zoom en un máximo de 2%  \n",
    "  \n",
    "Si bien no se contaba con un gran conjunto de imágenes, estás en promedio eran de un tamaño de 2970 * 2970 por lo cual, debido a las especificaciones de cada uno de los equipos de los integrantes del grupo no se podían cargar muchas imágenes en memoria, por lo tal se hizo el proceso por tandas, esta \"carga\" dinámica de los datos también se realiza mediante el generador. El tamaño de la tanda fue uno de los parámetros que se modificó para mejorar los resultados obtenidos por los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3W5heeiS3pZpiCm7rPOQ7c",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "AUG_TRAIN_ARGS = dict(rescale=1. / 255,  # normalization\n",
    "                      rotation_range=35,\n",
    "                      horizontal_flip=True,\n",
    "                      vertical_flip=True,\n",
    "                      width_shift_range=0.05,\n",
    "                      height_shift_range=0.05,\n",
    "                      zoom_range=0.2,\n",
    "                      fill_mode='nearest',\n",
    "                      validation_split=0.13)\n",
    "\n",
    "def augmentation(args: dict, data: DataFrame, path: str, size: int, x_col: str, batch: int, seed: int, subset: str):\n",
    "    datagen = ImageDataGenerator(**args)\n",
    "    generator = datagen.flow_from_dataframe(dataframe=data,\n",
    "                                            directory=path,\n",
    "                                            x_col=x_col,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode=None,\n",
    "                                            batch_size=batch,\n",
    "                                            seed=seed,\n",
    "                                            subset=subset)\n",
    "    return generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "WgG9zzMC9tBd5zQVNt0CWj",
     "type": "MD"
    }
   },
   "source": [
    "A pesar de que se estaba trabajando por tandas para el proceso de entrenamiento, si se trabajaba con un tamaño estandarizado de las imágenes de 2048 se llegaban a acabar los recursos. Por lo tal se decidió hacer una compresión de las imágenes, para esto primero se cambió el formato de las imágenes y se redujo el tamaño, al igual que con el *bacht size* se estuvo \"jugando\" con los valores desde 256, 512 y 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "X1WN0sQwrsgzUKE1yesMac",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def compress(images: [str], src_dir: str, dst_dir: str, width: int, height: int):\n",
    "    for index, image in tqdm(enumerate(images), total=len(images)):\n",
    "        img = Image.open(src_dir + image)\n",
    "        img = img.resize((width, height), Image.ANTIALIAS)\n",
    "        name = os.path.basename(image).split('.')[0]\n",
    "        img.save(f'{dst_dir}/{name}.png', quality=85, optimize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "FzhBC3TdfCL92QLyVVvIBz",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "## Ajuste de algoritmos\n",
    "Mediante las gráficas que muestran la evolución de los modelos en cuanto al aprendizaje se pudo diagnosticar el rendimiento y qué aspectos deberían de ser modificados para mejorar las predicciones. Para ello se utilizó el historial retornado al momento de entrenar el modelo, y mediante estas gráficas se sabe que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XH4NGK4VKMVYovDftvphYC",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def history_plots(history, metrics=['iou_score', 'val_iou_score'], loss=['loss', 'val_loss']):\n",
    "    train_metric = history.history[metrics[0]]\n",
    "    val_metric = history.history[metrics[1]]\n",
    "    train_loss = history.history[loss[0]]\n",
    "    val_loss = history.history[loss[1]]\n",
    "\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_metric, label='Training score')\n",
    "    plt.plot(epochs, val_metric, label='Validation Score')\n",
    "    plt.title('Training and validation score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GiQq7jPBIkSxhR2Umo74Bw",
     "type": "MD"
    }
   },
   "source": [
    "Otra estrategia y quiza una de las más utilizadas fue ir jugando con los parámetros, este \"juego\" se realizó tanto como con el número de épocas como con el *batch size*. Con esto se pudo observar que mientras se utilizara una mayor cantidad de épocas, se obtenía un mejor valor en la métrica utilizada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HncpBmhT2OEtk4k2KxzsTB",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RDHJCtm9GSKVeZuR1Fhj4R",
     "type": "MD"
    }
   },
   "source": [
    "## Comparación de algoritmos\n",
    "Comparación de los algoritmos de acuerdo con la efectividad, tiempos de procesamiento, errores, etc. Utiliza para esto, gráficos explicativos, estáticos con colores adecuados.\n",
    "\n",
    "El tiempo fue una de las métricas de comparación más importante en estos algoritmos, en general, se pueden considerar como modelos bastante \"pesados\" ya que trabajan con una serie de redes convolucionales en la arquitectura UNet y en el caso de la arquitectura Linknet trabaja con redes *fully connected*. Por lo tal, en promedio se puede decir que el modelo que menor tiempo tomo fue de aproximadamente unos 40min y fue utilizando la implementación de la librería segmentation models con los pesos de *efficientnetb0*. \n",
    "\n",
    "En este caso pudimos notar algo interesante y fue el caso del modelo que tomo más tiempo en entrenarse, lo cual fue aproximadamente 3 horas, con únicamente 10 épocas, este fue el modelo Linknet con pesos ya entrenados de tipo vegg16, con lo cual se puedo encontrar que una optimización de los pesos puede mejorar significativamente el *performance* en cuanto a tiempo de un modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wde8YpOmSJKALZYKmkieBt",
     "type": "MD"
    }
   },
   "source": [
    "A su vez, otra de las métricas, y quizá la principal para evaluar los algoritmos fue la comparación de las predicciones con respecto a las máscaras originales. Para ello se gráfico mediante Matplotlib, tanto las células, como las máscaras reales y predichas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "7U7yOvWducWCSeuH07n4dM",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def compare(segmentation_model, data: DataFrame, loop: int, treshold: float):\n",
    "    fig, ax = plt.subplots(loop, 3, figsize=(25, 25))\n",
    "    for x in range(loop):\n",
    "        img = Image.open(IMAGES_PATH + data[x])\n",
    "        img = test_preprocess(img)\n",
    "        mask = Image.open(MASKS_PATH + data[x])\n",
    "        prediction = segmentation_model.predict(img)\n",
    "        prediction = (prediction > treshold).astype(np.uint8)\n",
    "        ax[x][0].imshow(img[0, :, :, ])\n",
    "        ax[x][1].imshow(mask, cmap='gray')\n",
    "        ax[x][2].imshow(prediction[0, :, :, ], cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RTBxZhdLTN0rsyhHs7STZp",
     "type": "MD"
    }
   },
   "source": [
    "Finalmente, se compararon el nivel de *score* obtenidos por cada uno de los modelos para esto, una métrica muy utilizada en segmentación de imágenes es la *Intersection over Union* la cual se conoce muy comúnmente como IoU. Esta métrica consiste en..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "WCj1wd0854L7BxijOxCxvw",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "## Aplicación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "s71XjADfAAlsjzGuXY4Yge",
     "report_properties": {},
     "type": "MD"
    }
   },
   "source": [
    "# Referencias\n",
    "\n",
    "- Bandyopadhyay. H, (10/2022). An Introduction to Image Segmentation: Deep Learning vs. Traditional [+Examples]. https://www.v7labs.com/blog/image-segmentation-guide \n",
    "- Datagen (sf). Image Segmentation: The Basics and 5 Key Techniques. https://datagen.tech/guides/image-annotation/image-segmentation/ \n",
    "- GeeksForGeeks (07/2021). Image Segmentation using K Means Clustering. https://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/ \n",
    "- Iglesias. D, (05/2021). Segmentación de Imágenes con Redes Convolucionales. https://www.iartificial.net/segmentacion-imagenes-redes-convolucionales/#La_segmentacion_el_machine_learning_y_la_importancia_de_las_redes_convolucionales \n",
    "- KeepCodig, (01/2022). ¿Qué son las Redes Neuronales Convolucionales? https://keepcoding.io/blog/redes-neuronales-convolucionales/#Que_son_las_Redes_Neuronales_Convolucionales \n",
    "- Murzova. A (08/2020). Otsu’s Thresholding with OpenCV. https://learnopencv.com/otsu-thresholding-with-opencv/ \n",
    "- Sahir. S (01/2019). Canny Edge Detection Step by Step in Python — Computer Vision. https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123 \n",
    "- Shiledarbaxi, N (sf). Semantic vs Instance vs Panoptic: Which Image Segmentation Technique To Choose. https://analyticsindiamag.com/semantic-vs-instance-vs-panoptic-which-image-segmentation-technique-to-choose/ "
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "85aba615decdb3a01ede5f950161a0711e40dfddec8a67547c00f5b8562848d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
